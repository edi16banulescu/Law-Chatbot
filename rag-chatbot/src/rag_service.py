import sys
from google import genai
from src.data_processor import load_and_chunk_data
from src.vector_db_manager import create_or_update_db, retrieve_chunks, clear_db
import ollama
from dotenv import load_dotenv

load_dotenv()

# --- Configuratii LLM Local ---
GENERATION_MODEL = 'gemma2:9b' # Modelul de chat

def optimize_query_with_llm(user_query: str) -> str:
    """
    ReformuleazÄƒ Ã®ntrebarea folosind LLM local.
    """
    optimization_prompt = (
        "EÈ™ti un asistent specializat Ã®n legislaÈ›ia rutierÄƒ din RomÃ¢nia. "
        "Sarcina ta este sÄƒ REFORMULEZI Ã®ntrebarea utilizatorului pentru a fi gÄƒsitÄƒ uÈ™or Ã®n OUG 195/2002, HG 1391/2006 È™i Codul Penal (Art. 334-338).\n"
        "Reguli:\n"
        "1. ÃnlocuieÈ™te termenii colocviali cu termeni legali (ex: 'carnet' -> 'permis de conducere', 'bÄƒut' -> 'sub influenÈ›a alcoolului', 'dosar penal' -> 'infracÈ›iune').\n"
        "2. PÄƒstreazÄƒ sensul Ã®ntrebÄƒrii, dar fÄƒ-o sÄƒ sune ca un text de lege.\n"
        "3. ReturneazÄƒ DOAR Ã®ntrebarea reformulatÄƒ.\n\n"
        f"Ãntrebare Utilizator: {user_query}\n"
        "Ãntrebare OptimizatÄƒ:"
    )
    
    try:
        response = ollama.chat(model=GENERATION_MODEL, messages=[
            {'role': 'user', 'content': optimization_prompt}
        ])
        return response['message']['content'].strip()
    except Exception as e:
        print(f"[WARN] Ollama error: {e}")
        return user_query

def generate_response_with_llm(retrieved_chunks: list[dict], user_query: str) -> str:
    """
    GenereazÄƒ rÄƒspunsul final folosind Llama 3.2 local.
    """
    context_text = "\n".join([f"[{chunk['articol']}]: {chunk['text']}" for chunk in retrieved_chunks])
    citations = sorted(list(set([chunk['articol'] for chunk in retrieved_chunks])))
    citations_str = ", ".join(citations[:5]) 

    system_prompt = (
        "EÈ™ti un asistent juridic expert Ã®n Codul Rutier RomÃ¢n È™i InfracÈ›iuni Rutiere (OUG 195, HG 1391, Cod Penal). "
        "RÄƒspunde la Ã®ntrebarea utilizatorului bazÃ¢ndu-te **DOAR** pe CONTEXTUL FURNIZAT.\n"
        "- DacÄƒ fapta este o CONTRAVENÈšIE (amendÄƒ), specificÄƒ clasa de sancÈ›iuni sau punctele.\n"
        "- DacÄƒ fapta este o INFRACÈšIUNE (Ã®nchisoare), specificÄƒ pedeapsa conform Codului Penal din context.\n"
        "- CiteazÄƒ articolul de lege relevant.\n"
        "- DacÄƒ informaÈ›ia nu existÄƒ Ã®n context, spune clar 'Nu am gÄƒsit informaÈ›ia Ã®n articolele regÄƒsite'."
    )
    
    user_message = f"CONTEXT LEGISLATIV:\n---\n{context_text}\n---\n\nÃNTREBARE UTILIZATOR: {user_query}\n\nRÄ‚SPUNS:"
    
    try:
        response = ollama.chat(model=GENERATION_MODEL, messages=[
            {'role': 'system', 'content': system_prompt},
            {'role': 'user', 'content': user_message}
        ])
        
        final_answer = response['message']['content']
        if final_answer:
             final_answer += f"\n\n(Surse: {citations_str})"
        return final_answer
    except Exception as e:
        return f"Eroare generare localÄƒ: {e}"

# --- FUNCTII MODUL INTERACTIV ---

def initialize_rag_system():
    print("\n" + "="*60)
    print(" ğŸ¦™  INITIALIZARE AGENT RUTIER (LOCAL - OLLAMA)... ")
    print("="*60)
    
    reindex = input("DoreÈ™ti re-indexarea completÄƒ a bazei de date? (da/nu) [nu]: ").lower().strip()
    
    if reindex in ['da', 'y', 'yes']:
        print("... È˜tergerea bazei de date vechi ...")
        try:
            from vector_db_manager import clear_db
            clear_db()
        except ImportError:
            pass

    print("... ÃncÄƒrcare date ...")
    chunks_list, metadata_list, document_ids = load_and_chunk_data()
    
    if not chunks_list:
        print("[EROARE] Nu s-au putut Ã®ncÄƒrca datele.")
        sys.exit(1)

    print(f"... Conectare la ChromaDB si Vectorizare LocalÄƒ...")
    collection = create_or_update_db(chunks_list, metadata_list, document_ids)
    
    print("\nâœ… Sistem local pregÄƒtit!")
    return collection

def process_query(collection, user_input, k_results=10): # K=10 e suficient pt Llama
    print(" ğŸ¦™ (GÃ¢ndesc...) Reformulez Ã®ntrebarea...")
    enhanced_query = optimize_query_with_llm(user_input)
    
    print(" ğŸ” (Caut...) Analizez legislaÈ›ia...")
    retrieved_chunks = retrieve_chunks(collection, enhanced_query, k=k_results)
    
    if not retrieved_chunks:
        return "Nu am gÄƒsit articole relevante."

    print(" âœï¸  (Scriu...) Generez rÄƒspunsul...")
    answer = generate_response_with_llm(retrieved_chunks, user_input)
    return answer

def start_interactive_chat():
    try:
        collection = initialize_rag_system()
    except Exception as e:
        print(f"[FATAL] {e}")
        return

    print("\n" + "!" * 60)
    print(" MOD LOCAL ACTIVAT (Llama 3.2 + Nomic Embed)")
    print("!" * 60)
    
    while True:
        try:
            user_input = input("\nTu: ").strip()
            if user_input.lower() in ['exit', 'q']: break
            print("\nAgent:", process_query(collection, user_input))
            print("-" * 60)
        except KeyboardInterrupt: break

if __name__ == "__main__":
    start_interactive_chat()
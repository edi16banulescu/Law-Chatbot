import os
import sys
from google import genai
from google.genai.errors import APIError
from data_processor import load_and_chunk_data

# --- Configuratii LLM ---
GENERATION_MODEL = 'gemini-2.5-flash'

def get_gemini_client():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY nu este setatÄƒ.")
    return genai.Client(api_key=api_key)

def optimize_query_with_llm(user_query: str) -> str:
    """
    ReformuleazÄƒ Ã®ntrebarea utilizatorului Ã®n limbaj juridic folosind LLM.
    """
    client = get_gemini_client()
    
    optimization_prompt = (
        "EÈ™ti un asistent specializat Ã®n legislaÈ›ia rutierÄƒ din RomÃ¢nia. "
        "Sarcina ta este sÄƒ REFORMULEZI Ã®ntrebarea utilizatorului pentru a fi gÄƒsitÄƒ uÈ™or Ã®n OUG 195/2002, HG 1391/2006 È™i Codul Penal (Art. 334-338).\n"
        "Reguli:\n"
        "1. ÃnlocuieÈ™te termenii colocviali cu termeni legali (ex: 'carnet' -> 'permis de conducere', 'bÄƒut' -> 'sub influenÈ›a alcoolului', 'dosar penal' -> 'infracÈ›iune').\n"
        "2. PÄƒstreazÄƒ sensul Ã®ntrebÄƒrii, dar fÄƒ-o sÄƒ sune ca un text de lege.\n"
        "3. ReturneazÄƒ DOAR Ã®ntrebarea reformulatÄƒ.\n\n"
        f"Ãntrebare Utilizator: {user_query}\n"
        "Ãntrebare OptimizatÄƒ:"
    )
    
    try:
        response = client.models.generate_content(
            model=GENERATION_MODEL,
            contents=[optimization_prompt]
        )
        return response.text.strip()
    except Exception as e:
        return user_query

def generate_response_with_llm(retrieved_chunks: list[dict], user_query: str) -> str:
    """
    GenereazÄƒ rÄƒspunsul final bazat pe context.
    """
    client = get_gemini_client()
    
    context_text = "\n".join([f"[{chunk['articol']}]: {chunk['text']}" for chunk in retrieved_chunks])
    citations = sorted(list(set([chunk['articol'] for chunk in retrieved_chunks])))
    citations_str = ", ".join(citations[:5]) 

    system_prompt = (
        "EÈ™ti un asistent juridic expert Ã®n Codul Rutier RomÃ¢n È™i InfracÈ›iuni Rutiere (OUG 195, HG 1391, Cod Penal). "
        "RÄƒspunde la Ã®ntrebarea utilizatorului bazÃ¢ndu-te **DOAR** pe CONTEXTUL FURNIZAT.\n"
        "- DacÄƒ fapta este o CONTRAVENÈšIE (amendÄƒ), specificÄƒ clasa de sancÈ›iuni sau punctele.\n"
        "- DacÄƒ fapta este o INFRACÈšIUNE (Ã®nchisoare), specificÄƒ pedeapsa conform Codului Penal din context.\n"
        "- CiteazÄƒ articolul de lege relevant.\n"
        "- DacÄƒ informaÈ›ia nu existÄƒ Ã®n context, spune clar 'Nu am gÄƒsit informaÈ›ia Ã®n articolele regÄƒsite'."
    )
    
    prompt = f"CONTEXT LEGISLATIV:\n---\n{context_text}\n---\n\nÃNTREBARE UTILIZATOR: {user_query}\n\nRÄ‚SPUNS:"
    
    try:
        response = client.models.generate_content(
            model=GENERATION_MODEL,
            contents=[prompt],
            config={'system_instruction': system_prompt}
        )
        final_answer = response.text
        if final_answer:
             final_answer += f"\n\n(Surse: {citations_str})"
        return final_answer
    except Exception as e:
        return f"Eroare generare: {e}"

# --- FUNCTII MODUL INTERACTIV ---

def initialize_rag_system():
    print("\n" + "="*60)
    print(" ğŸš—  INITIALIZARE AGENT RUTIER... ")
    print("="*60)
    
    reindex = input("DoreÈ™ti re-indexarea completÄƒ a bazei de date? (da/nu) [nu]: ").lower().strip()
    
    if reindex in ['da', 'y', 'yes']:
        print("... È˜tergerea bazei de date vechi ...")
        try:
            from vector_db_manager import clear_db
            clear_db()
        except ImportError:
            pass

    print("... ÃncÄƒrcare È™i verificare BazÄƒ de CunoÈ™tinÈ›e ...")
    chunks_list, metadata_list, document_ids = load_and_chunk_data()
    
    if not chunks_list:
        print("[EROARE] Nu s-au putut Ã®ncÄƒrca datele. VerificÄƒ 'codul_rutier.txt'.")
        sys.exit(1)

    print(f"... Conectare la ChromaDB ({len(chunks_list)} segmente)...")
    from vector_db_manager import create_or_update_db
    collection = create_or_update_db(chunks_list, metadata_list, document_ids)
    
    print("\nâœ… Sistem pregÄƒtit!")
    return collection

def process_query(collection, user_input, k_results=15):
    # 1. Optimizare
    print(" ğŸ¤– (GÃ¢ndesc...) Reformulez Ã®ntrebarea...")
    enhanced_query = optimize_query_with_llm(user_input)
    
    # 2. Retrieval
    print(" ğŸ” (Caut...) Analizez legislaÈ›ia...")
    from vector_db_manager import retrieve_chunks
    retrieved_chunks = retrieve_chunks(collection, enhanced_query, k=k_results)
    
    if not retrieved_chunks:
        return "Nu am gÄƒsit articole relevante Ã®n baza de date."

    # 3. Generation
    print(" âœï¸  (Scriu...) Generez rÄƒspunsul...")
    answer = generate_response_with_llm(retrieved_chunks, user_input)
    return answer

def start_interactive_chat():
    try:
        vector_db_collection = initialize_rag_system()
    except Exception as e:
        print(f"[FATAL] Eroare la iniÈ›ializare: {e}")
        return

    # --- DISCLAIMER LEGAL ---
    print("\n" + "!" * 60)
    print(" AVERTISMENT LEGAL:")
    print(" Acest asistent este un proiect academic demonstrativ.")
    print(" InformaÈ›iile oferite nu reprezintÄƒ consultanÈ›Äƒ juridicÄƒ oficialÄƒ.")
    print(" VerificaÈ›i Ã®ntotdeauna legea Ã®n vigoare sau consultaÈ›i un avocat.")
    print("!" * 60)
    
    print("\nScrie 'exit' sau 'q' pentru a Ã®nchide.")
    print("-" * 60)

    while True:
        try:
            user_input = input("\nTu: ").strip()
            
            if not user_input:
                continue
                
            if user_input.lower() in ['exit', 'quit', 'q']:
                print("La revedere! Drum bun! ğŸš—")
                break
            
            response = process_query(vector_db_collection, user_input)
            
            print("\nAgent Rutier:")
            print(response)
            print("-" * 60)
            
        except KeyboardInterrupt:
            print("\nLa revedere!")
            break
        except Exception as e:
            print(f"\n[Eroare]: {e}")

if __name__ == "__main__":
    if not os.getenv("GEMINI_API_KEY"):
        print("EROARE: Variabila de mediu GEMINI_API_KEY nu este setatÄƒ!")
    else:
        start_interactive_chat()